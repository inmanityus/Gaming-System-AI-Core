# Available Ollama Models Inventory
**Date**: January 29, 2025  
**Status**: ‚úÖ Verified and Configured

---

## AVAILABLE MODELS ANALYSIS

### ‚úÖ Tier 1 Models (Generic NPCs - Zombies, Ghouls)

**Recommended for Tier 1**:
- ‚úÖ **phi3:mini** (2.2 GB) - ‚≠ê **BEST CHOICE**
  - Size: Perfect for Tier 1
  - Quality: Excellent for simple dialogue
  - Latency: 50-150ms expected
  
- ‚úÖ **tinyllama** (637 MB) - ‚≠ê **ULTRA-LIGHT**
  - Size: Smallest option
  - Quality: Good for grunts, taunts
  - Latency: Fastest (30-100ms expected)
  
- ‚úÖ **qwen2.5:3b** (1.9 GB) - ‚úÖ **GOOD**
  - Quality: Better than TinyLlama
  - Latency: 50-120ms expected
  
- ‚úÖ **llama3.2:3b** (2.0 GB) - ‚úÖ **GOOD**
  - Quality: Good instruction following
  - Latency: 50-120ms expected

**Use Case**: Simple reactions, grunts, basic dialogue
**Concurrency**: 10-20 NPCs per GPU
**VRAM Usage**: 1-2 GB per instance

---

### ‚úÖ Tier 2 Models (Elite NPCs - Vampires, Werewolves)

**Recommended for Tier 2** (with LoRA adapters):
- ‚úÖ **llama3.1:8b** (4.9 GB) - ‚≠ê **BEST CHOICE**
  - Quality: Excellent for Tier 2
  - LoRA Support: Full support
  - Latency: 100-300ms expected
  
- ‚úÖ **mistral:7b** (4.4 GB) - ‚≠ê **EXCELLENT**
  - Quality: Great instruction following
  - LoRA Support: Full support
  - Latency: 100-250ms expected
  
- ‚úÖ **qwen2.5:7b** (4.7 GB) - ‚úÖ **GOOD**
  - Quality: Good, multilingual
  - LoRA Support: Full support
  - Latency: 100-280ms expected
  
- ‚úÖ **mistral-openorca:7b** (4.1 GB) - ‚úÖ **ALTERNATIVE**
  - Quality: Orca-trained variant
  - Use: Alternative if standard Mistral unavailable

**Use Case**: Threats, negotiations, contextual reactions
**Concurrency**: 5-10 NPCs per GPU
**VRAM Usage**: 4-6 GB per instance (with LoRA)

---

### ‚úÖ Tier 3 Models (Major NPCs - Nemeses, Bosses)

**Base Models** (same as Tier 2, with personalized LoRA):
- ‚úÖ **llama3.1:8b** + Personalized LoRA - ‚≠ê **PRIMARY**
- ‚úÖ **mistral:7b** + Personalized LoRA - ‚≠ê **ALTERNATIVE**

**Use Case**: Full conversations, personality depth
**Concurrency**: 2-5 NPCs per GPU
**VRAM Usage**: 6-8 GB per instance (with personalized LoRA)

---

### ‚ö†Ô∏è Specialized Models (Not for Standard NPCs)

- ‚úÖ **deepseek-coder-v2** (8.9 GB)
  - Use: Code generation, specialized tasks
  - NOT for NPC dialogue
  
- ‚úÖ **deepseek-r1** (5.2 GB) - ‚ö†Ô∏è **NOTE: NOT DeepSeek V3**
  - Use: Reasoning tasks, complex logic
  - NOT DeepSeek V3 (user cannot support V3)
  - Still useful for reasoning-heavy NPCs
  
- ‚úÖ **qwen2.5-coder:7b** (4.7 GB)
  - Use: Code generation, specialized tasks
  - NOT for standard NPC dialogue

- ‚úÖ **hir0rameel/qwen-claude:latest** (5.2 GB)
  - Use: Experimental, Claude-like behavior
  - Could be interesting for Tier 2/3 NPCs

---

### ‚ùå Large Models (Too Big for Production)

- ‚ùå **gemma3:27b** (17 GB) - Too large for concurrent use
- ‚ùå **magistral:24b** (14 GB) - Too large for concurrent use
- ‚ùå **llama2-uncensored:70b** (38 GB) - Far too large
- ‚ùå **qwen3:235b-a22b** (142 GB) - Massive, impractical

**Recommendation**: Avoid these for production NPC serving

---

### ü§î Experimental Models

- ‚úÖ **gpt-oss** (13 GB)
  - Use: Experimental, may be useful for Tier 3
  - Test performance before production use

---

## UPDATED MODEL ALLOCATION

### Recommended Configuration

**Tier 1 (Generic NPCs)**:
```yaml
Primary: phi3:mini (2.2 GB)
Fallback: tinyllama (637 MB) - for ultra-fast responses
Alternative: qwen2.5:3b (1.9 GB) - for better quality when needed
```

**Tier 2 (Elite NPCs)**:
```yaml
Primary: llama3.1:8b (4.9 GB) - Best overall
Secondary: mistral:7b (4.4 GB) - Excellent alternative
Fallback: qwen2.5:7b (4.7 GB) - Good quality
```

**Tier 3 (Major NPCs)**:
```yaml
Primary: llama3.1:8b + Personalized LoRA
Alternative: mistral:7b + Personalized LoRA
```

**Specialized Tasks**:
```yaml
Reasoning: deepseek-r1 (5.2 GB) - Complex logic
Coding: deepseek-coder-v2 (8.9 GB) - Code generation
Experimental: gpt-oss (13 GB) - Test for Tier 3 alternative
```

---

## DEEPSEEK NOTE

‚ö†Ô∏è **Important**: You have **deepseek-r1** but **NOT DeepSeek V3.1**

- **Available**: `deepseek-r1` (5.2 GB) - Reasoning model
- **NOT Available**: DeepSeek V3.1 (as specified in some docs)
- **Alternative**: Use DeepSeek V3.1 via Azure deployment or Direct API

**Recommendation**:
- Use `deepseek-r1` locally for reasoning tasks
- Use DeepSeek V3.1 via Azure/API for orchestration when needed
- Update documentation to reflect this

---

## VRAM CAPACITY ANALYSIS

**Your Hardware**: 2x RTX 5090 (32GB each) = 64GB total VRAM

**Potential Load**:
- Tier 1: 10-20 instances √ó 1.5GB = 15-30GB
- Tier 2: 5-10 instances √ó 5GB = 25-50GB
- Tier 3: 2-5 instances √ó 7GB = 14-35GB

**Total Capacity**: With 64GB VRAM, you can handle:
- ‚úÖ Full Tier 1 + Tier 2 deployment on one GPU
- ‚úÖ Tier 3 instances on second GPU
- ‚úÖ Plenty of headroom for LoRA adapters

---

## NEXT STEPS

1. ‚úÖ **Model inventory complete** - All models documented
2. ‚è≠Ô∏è **Test model performance** - Benchmark latency/quality
3. ‚è≠Ô∏è **Create LoRA adapters** - Train per monster type
4. ‚è≠Ô∏è **Update solution docs** - Remove DeepSeek V3.1 references (local)
5. ‚è≠Ô∏è **Configure model routing** - Set up tier-based selection

---

**Status**: ‚úÖ Ready for implementation  
**Note**: DeepSeek V3.1 available via Azure/API, not locally

