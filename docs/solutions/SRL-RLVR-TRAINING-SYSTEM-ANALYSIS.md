# SRLâ†’RLVR Training System - Phase 1: Analysis & Decomposition
**Date**: 2025-01-29  
**Status**: Phase 1 - Analysis  
**Research Sources**: 
- Google SRL Paper: arXiv:2510.25992 (October 2025)
- Perplexity Research: SRL + RLVR Integration
- Marktechpost Article: Google AI SRL Framework

---

## ðŸš¨ EXECUTIVE SUMMARY

This document initiates the complex solution process for implementing Google's Supervised Reinforcement Learning (SRL) â†’ Reinforcement Learning with Verifiable Rewards (RLVR) training pipeline for small self-hosted models in the gaming platform.

### **Research Findings:**

**Google SRL (Supervised Reinforcement Learning)**:
- Injects supervision into reward channels (not loss function)
- Step-wise action generation with reasoning monologue
- Dense rewards via sequence similarity (difflib-based)
- Enables small models to learn from hard reasoning tasks
- Validated on Qwen2.5 7B Instruct (s1K 1.1 math problems)

**RLVR (Reinforcement Learning with Verifiable Rewards)**:
- Outcome-level RL that rewards correct solutions
- Fails when correct rollouts are rare
- Best performance: SRL first â†’ then RLVR

**Key Innovation**:
- SRL provides step-wise learning signals even when final answer is wrong
- RLVR reinforces correct solutions after SRL establishes foundation
- Combined pipeline: Qwen 7B â†’ SRL â†’ RLVR = Best open-source scores

---

## 1. REQUIREMENT ANALYSIS

### 1.1 User Requirements

**Primary Goal**: 
Train small self-hosted models (Qwen 7B Instruct/Coder) using SRLâ†’RLVR pipeline for gaming platform needs.

**Specific Requirements**:
1. **Three-Model Collaboration System**:
   - Model 1: Gather all lore for a given monster species
   - Model 2: Create examples relevant to our games
   - Model 3: Generate training examples for SRL pipeline
   
2. **Dynamic Rules Integration**:
   - Rules change based on game state, player actions, lore updates
   - System must adapt training examples to current rules
   
3. **Monster-Specific Training**:
   - Train models for specific monster types (vampires, werewolves, zombies, ghouls, liches)
   - Each monster type has unique behavior patterns, dialogue styles, decision-making
   
4. **Continuous Growth**:
   - Models will always be growing over time
   - System must support incremental updates, not just one-time training
   - Historical logs feed back into training pipeline

5. **Integration with Existing System**:
   - Must work with existing Model Management System
   - Must integrate with LoRA adapter strategy
   - Must support Tier 1/2/3 NPC models

### 1.2 Technical Constraints

**Model Constraints**:
- Base models: Qwen2.5-7B-Instruct, Qwen2.5-7B-Coder
- Hardware: Self-hosted (local inference servers)
- Quantization: 8-bit/FP8 preferred, 4-bit for VRAM constraints
- Latency: Must maintain <500ms TTFT for Tier 3 NPCs

**Training Constraints**:
- Training data: AI-generated examples (not real player data initially)
- Expert trajectories: Generated by 3-model collaboration system
- Training frequency: Incremental updates (not full retraining each time)
- Resource limits: Training on AWS SageMaker (per AWS deployment rule)

**Integration Constraints**:
- Must work with existing LoRA adapter system
- Must support blue-green deployment
- Must maintain guardrails (immersive but not harmful)
- Must integrate with historical log system

### 1.3 Gaming Platform Context

**Current Model Usage**:
- **Tier 1**: Generic monsters (3-4B models) - simple reactions
- **Tier 2**: Elite monsters (7-8B + LoRA) - negotiations, threats
- **Tier 3**: Major NPCs (7-8B + personalized LoRA) - full conversations

**Monster Types Needing Training**:
1. Vampires: Aristocratic, centuries-old, disdain for humans
2. Werewolves: Pack mentality, territorial, primal
3. Zombies: Basic intelligence, hunger-driven
4. Ghouls: Macabre humor, body part obsessed
5. Liches: Ancient wisdom, magical, calculating

**Game Lore Sources**:
- Centralized game state (PostgreSQL)
- Semantic memory (Vector DB)
- Historical player interactions
- Game narrative documents

---

## 2. PROBLEM DECOMPOSITION

### 2.1 Core Components Identified

The SRLâ†’RLVR training system breaks down into these main components:

#### **A. Expert Trajectory Generation System** (3-Model Collaboration)
- **Purpose**: Generate expert trajectories for SRL training
- **Models**: 3 top models working collaboratively
- **Output**: Structured expert trajectories (step-by-step solutions)

#### **B. SRL Training Pipeline**
- **Purpose**: Train models using step-wise supervised rewards
- **Input**: Expert trajectories, base model, dynamic rules
- **Output**: SRL-trained model (intermediate checkpoint)

#### **C. RLVR Fine-Tuning Pipeline**
- **Purpose**: Further refine SRL-trained models with outcome rewards
- **Input**: SRL checkpoint, verifiable correct solutions
- **Output**: Final trained model ready for deployment

#### **D. Dynamic Rules Engine**
- **Purpose**: Generate training examples based on current game rules
- **Input**: Game state, lore, monster species data
- **Output**: Context-specific training rules and examples

#### **E. Incremental Update System**
- **Purpose**: Update models continuously as game evolves
- **Input**: New lore, player feedback, performance metrics
- **Output**: Updated model versions

#### **F. Integration Layer**
- **Purpose**: Integrate with existing model management system
- **Components**: LoRA adapter integration, deployment, monitoring

### 2.2 Component Dependencies

```
Expert Trajectory Generation
    â†“
SRL Training Pipeline
    â†“
RLVR Fine-Tuning Pipeline
    â†“
Integration Layer â†’ Deployment

Dynamic Rules Engine â”€â”€â†’ All Components
Incremental Update System â”€â”€â†’ All Components
```

### 2.3 Data Flow

1. **Lore Collection**: Gather all lore for monster species
2. **Rule Generation**: Generate dynamic rules based on game state
3. **Example Creation**: 3-model system creates training examples
4. **Trajectory Parsing**: Parse examples into step-wise actions
5. **SRL Training**: Train with step-wise rewards
6. **RLVR Fine-Tuning**: Fine-tune with outcome rewards
7. **Deployment**: Integrate with existing model management
8. **Monitoring**: Track performance, collect feedback
9. **Iteration**: Feed back into system for continuous improvement

---

## 3. INITIAL STRUCTURE

### 3.1 Proposed Solution Architecture

```
SRL-RLVR Training System
â”œâ”€â”€ Expert Trajectory Generation
â”‚   â”œâ”€â”€ Lore Aggregation Module
â”‚   â”œâ”€â”€ Three-Model Collaboration System
â”‚   â””â”€â”€ Trajectory Parser
â”œâ”€â”€ Training Pipeline
â”‚   â”œâ”€â”€ SRL Trainer
â”‚   â”œâ”€â”€ RLVR Fine-Tuner
â”‚   â””â”€â”€ Checkpoint Manager
â”œâ”€â”€ Dynamic Rules Engine
â”‚   â”œâ”€â”€ Rule Generator
â”‚   â”œâ”€â”€ Context Builder
â”‚   â””â”€â”€ Example Validator
â”œâ”€â”€ Incremental Update System
â”‚   â”œâ”€â”€ Change Detection
â”‚   â”œâ”€â”€ Training Data Updater
â”‚   â””â”€â”€ Model Version Manager
â””â”€â”€ Integration Layer
    â”œâ”€â”€ LoRA Adapter Generator
    â”œâ”€â”€ Deployment Manager
    â””â”€â”€ Performance Monitor
```

### 3.2 Technology Stack (Initial)

**Training Infrastructure**:
- AWS SageMaker (per AWS deployment rule)
- PyTorch / Transformers
- PEFT (LoRA adapters)
- Accelerate (distributed training)

**Model Collaboration**:
- OpenRouter AI MCP (for multi-model access)
- Director model: GPT-5 Pro or Claude 4.5 Sonnet
- Reviewer models: Mix of top models (meeting minimum levels)

**Data Management**:
- PostgreSQL (lore, game state)
- Vector DB (semantic memory)
- S3 (training data, checkpoints)

**Integration**:
- Existing Model Management System
- LoRA adapter framework
- Blue-green deployment system

---

## 4. NEXT PHASE: SOLUTION ARCHITECTURE

**Phase 2 will develop**:
1. Detailed architecture for each component
2. Best practices research for SRL/RLVR implementation
3. Integration with AWS SageMaker
4. Security and performance considerations
5. Peer-reviewed solutions using 5-model collaboration

**Models to Use for Phase 2**:
1. **Director**: GPT-5 Pro or Claude 4.5 Sonnet (consolidation)
2. **Reviewer 1**: Claude 4.5 Sonnet (different provider from Director)
3. **Reviewer 2**: DeepSeek V3.1 Terminus (reasoning specialist)
4. **Reviewer 3**: Gemini 2.5 Pro (research and validation)
5. **Reviewer 4**: Grok 4 or Qwen 3 Max (diversity)

---

**Status**: Phase 1 Complete  
**Next**: Phase 2 - Solution Architecture (awaiting 5-model collaboration)

