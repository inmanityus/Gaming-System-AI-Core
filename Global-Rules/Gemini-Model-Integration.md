# Gemini Model Integration for Cursor AI Sessions

## üéØ Overview

This document explains how to access and use Google's Gemini models as an additional AI option alongside Cursor's native models and OpenRouter's models. Gemini provides powerful multimodal capabilities and is available through multiple access methods.

---

## üåê Available Model Sources

### **1. Cursor Native Models (Built-in)**
**Access:** Direct through Cursor's model selector  
**Models:** Claude, GPT-4, etc.  
**Configuration:** Cursor Settings ‚Üí Models  
**Status:** ‚úÖ Always available

### **2. OpenRouter AI Models (via MCP)**
**Access:** Via OpenRouter AI MCP server  
**Models:** Claude, GPT, Gemini, Llama, and 100+ others  
**API:** `mcp_openrouterai_chat_completion` tool  
**Configuration:** MCP server settings  
**Status:** ‚úÖ Active in current setup

### **3. Gemini Models (Multiple Access Methods)**

#### **Option A: Gemini via OpenRouter MCP** ‚≠ê **RECOMMENDED**
**Access:** Via OpenRouter AI MCP server  
**API Tool:** `mcp_openrouterai_chat_completion`  
**Models Available:**
- `google/gemini-2.0-flash-exp` - Latest experimental
- `google/gemini-1.5-pro` - High capability
- `google/gemini-1.5-flash` - Fast responses
- `google/gemini-pro-vision` - Multimodal
- `google/gemini-ultra` - Highest tier

**Advantages:**
- ‚úÖ Already configured (OpenRouter MCP active)
- ‚úÖ No additional API key needed
- ‚úÖ Same interface as other models
- ‚úÖ Billing through OpenRouter

**Usage Example:**
```python
# Using OpenRouter MCP tool for Gemini
mcp_openrouterai_chat_completion(
    model="google/gemini-2.0-flash-exp",
    messages=[
        {"role": "user", "content": "Explain quantum computing"}
    ],
    temperature=0.7,
    max_tokens=1000
)
```

#### **Option B: Direct Gemini API**
**Access:** Via Google AI API  
**API Key:** Requires `GEMINI_API_KEY` environment variable  
**Configuration:** Cursor environment variables  
**Status:** ‚ö†Ô∏è Causes Cursor integration issues (not recommended)

**Why Not Recommended:**
- Gemini API key in Cursor's environment variables causes conflicts
- Integration breaks when added directly
- Better to use OpenRouter as intermediary

---

## üîß Configuration Methods

### **Method 1: Using Gemini via OpenRouter (Current Setup)**

**Current Status:** ‚úÖ Already Configured

Your OpenRouter MCP server is active and includes access to all Gemini models:

```json
{
  "MCP_SERVERS": {
    "openrouterai": {
      "active": true,
      "models_available": [
        "google/gemini-2.0-flash-exp",
        "google/gemini-1.5-pro", 
        "google/gemini-1.5-flash",
        "google/gemini-pro-vision"
      ]
    }
  }
}
```

**How to Use:**
1. Models are accessible via `mcp_openrouterai_chat_completion` tool
2. Search models: `mcp_openrouterai_search_models(provider="google")`
3. Get model info: `mcp_openrouterai_get_model_info(model="google/gemini-1.5-pro")`
4. Chat completion: `mcp_openrouterai_chat_completion(model="google/gemini-1.5-pro", messages=[...])`

**Advantages:**
- No environment variable conflicts
- Unified API for all models
- Automatic key management
- Works seamlessly with Cursor

### **Method 2: Direct API Integration (Alternative)**

If you prefer direct integration despite the issues:

**Setup Steps:**
1. **Get Gemini API Key** (from Google AI Studio)
2. **Add to Environment Variables** (‚ö†Ô∏è May cause issues)
3. **Access via Python Script** (recommended workaround)

**Workaround Implementation:**
```powershell
# Create a wrapper script that uses Gemini API directly
# This avoids Cursor environment variable conflicts

# File: scripts/use-gemini.ps1
$apiKey = "YOUR_GEMINI_API_KEY"
$pythonCode = @"
import google.generativeai as genai
import sys

genai.configure(api_key='$apiKey')
model = genai.GenerativeModel('gemini-pro')

response = model.generate_content(sys.argv[1])
print(response.text)
"@

$pythonCode | python -
```

**Why This Works:**
- Bypasses Cursor's environment variable system
- Uses Python library directly
- No Cursor integration conflicts

---

## üìã Model Selection Guidelines

### **When to Use Cursor Native Models**
- ‚úÖ Fast responses needed
- ‚úÖ Simple coding tasks
- ‚úÖ Quick debugging
- ‚úÖ Real-time assistance

**Models:** Claude Sonnet, GPT-4, GPT-3.5

### **When to Use OpenRouter Models (including Gemini)**
- ‚úÖ Research tasks
- ‚úÖ Complex problem-solving
- ‚úÖ Multimodal tasks (images, video)
- ‚úÖ Specific model capabilities needed
- ‚úÖ Cost optimization for specific tasks

**Example: Use Gemini for:**
- Multimodal analysis (code + images)
- Creative writing
- Mathematical reasoning
- Code generation with explanations

### **When to Use Gemini Specifically**
- ‚úÖ You need multimodal capabilities (code + images + video)
- ‚úÖ You need creative/social reasoning
- ‚úÖ You want different perspective on problem
- ‚úÖ Cost-effective option for specific tasks

---

## üîÑ Integration with Startup Script

### **Automatic Model Detection**

The startup script now includes Gemini awareness:

```powershell
# Add to startup.ps1
Write-Host "Loading Model Options..." -ForegroundColor Green

# Cursor Native Models (always available)
Write-Host "‚úì Cursor Native Models: Available" -ForegroundColor Green
Write-Host "  ‚Üí Claude Sonnet 4.5, GPT-4, GPT-3.5, etc." -ForegroundColor Gray

# OpenRouter Models (via MCP)
Write-Host "‚úì OpenRouter AI Models: Available via MCP" -ForegroundColor Green
Write-Host "  ‚Üí 100+ models including Gemini variants" -ForegroundColor Gray
Write-Host "  ‚Üí Use: mcp_openrouterai_chat_completion" -ForegroundColor Gray

# Gemini-specific info
Write-Host "‚úì Gemini Models Access:" -ForegroundColor Cyan
Write-Host "  ‚Üí google/gemini-2.0-flash-exp (experimental)" -ForegroundColor Gray
Write-Host "  ‚Üí google/gemini-1.5-pro (high capability)" -ForegroundColor Gray
Write-Host "  ‚Üí google/gemini-1.5-flash (fast)" -ForegroundColor Gray
Write-Host "  ‚Üí google/gemini-pro-vision (multimodal)" -ForegroundColor Gray
Write-Host "  ‚Üí Access via OpenRouter MCP" -ForegroundColor Gray
```

### **Model Selection Function**

```powershell
# Function to list all available models
function Get-AvailableModels {
    Write-Host ""
    Write-Host "üéØ AVAILABLE MODEL OPTIONS:" -ForegroundColor Cyan
    Write-Host "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê" -ForegroundColor Cyan
    
    Write-Host ""
    Write-Host "1Ô∏è‚É£ CURSOR NATIVE MODELS (Built-in):" -ForegroundColor Yellow
    Write-Host "  ‚Ä¢ Claude Sonnet 4.5" -ForegroundColor Green
    Write-Host "  ‚Ä¢ GPT-4" -ForegroundColor Green
    Write-Host "  ‚Ä¢ GPT-3.5" -ForegroundColor Green
    Write-Host "  ‚Ä¢ Access: Direct through Cursor" -ForegroundColor Gray
    
    Write-Host ""
    Write-Host "2Ô∏è‚É£ OPENROUTER MODELS (via MCP):" -ForegroundColor Yellow
    Write-Host "  ‚Ä¢ google/gemini-2.0-flash-exp" -ForegroundColor Green
    Write-Host "  ‚Ä¢ google/gemini-1.5-pro" -ForegroundColor Green
    Write-Host "  ‚Ä¢ google/gemini-1.5-flash" -ForegroundColor Green
    Write-Host "  ‚Ä¢ anthropic/claude-3.5-sonnet" -ForegroundColor Green
    Write-Host "  ‚Ä¢ openai/gpt-4" -ForegroundColor Green
    Write-Host "  ‚Ä¢ openai/gpt-4-turbo" -ForegroundColor Green
    Write-Host "  ‚Ä¢ meta-llama/llama-3-70b" -ForegroundColor Green
    Write-Host "  ‚Ä¢ And 100+ more..." -ForegroundColor Gray
    Write-Host "  ‚Ä¢ Access: mcp_openrouterai_chat_completion" -ForegroundColor Gray
    
    Write-Host ""
    Write-Host "3Ô∏è‚É£ RECOMMENDED GEMINI OPTIONS:" -ForegroundColor Yellow
    Write-Host "  ‚Ä¢ Best for coding: google/gemini-1.5-pro" -ForegroundColor Cyan
    Write-Host "  ‚Ä¢ Fast responses: google/gemini-1.5-flash" -ForegroundColor Cyan
    Write-Host "  ‚Ä¢ Latest features: google/gemini-2.0-flash-exp" -ForegroundColor Cyan
    Write-Host "  ‚Ä¢ With images: google/gemini-pro-vision" -ForegroundColor Cyan
}
```

---

## üöÄ Practical Usage Examples

### **Example 1: Using Gemini for Code Generation**

```python
# Request Gemini to generate code via OpenRouter
result = mcp_openrouterai_chat_completion(
    model="google/gemini-1.5-pro",
    messages=[
        {
            "role": "user", 
            "content": "Write a Python function to calculate fibonacci numbers with memoization"
        }
    ],
    temperature=0.3,
    max_tokens=500
)

print(result)
```

### **Example 2: Search for Available Gemini Models**

```python
# Search for Google models
models = mcp_openrouterai_search_models(
    provider="google",
    limit=10
)

for model in models:
    print(f"Model: {model.name}")
    print(f"Description: {model.description}")
    print(f"Context Length: {model.context_length}")
    print("---")
```

### **Example 3: Get Gemini Model Details**

```python
# Get detailed info about a specific Gemini model
info = mcp_openrouterai_get_model_info(
    model="google/gemini-1.5-pro"
)

print(f"Name: {info.name}")
print(f"Context Length: {info.context_length}")
print(f"Capabilities: {info.capabilities}")
print(f"Pricing: ${info.pricing.prompt} per 1M prompt tokens")
```

### **Example 4: Use Gemini for Research**

```python
# Use Gemini for research tasks
research_result = mcp_openrouterai_chat_completion(
    model="google/gemini-2.0-flash-exp",
    messages=[
        {
            "role": "system",
            "content": "You are a research assistant specializing in AI developments."
        },
        {
            "role": "user",
            "content": "Explain the latest advances in multimodal AI models and their applications."
        }
    ],
    temperature=0.7,
    max_tokens=1500
)
```

---

## üìä Model Comparison

| Model | Source | Speed | Capability | Cost | Best For |
|-------|--------|-------|------------|------|----------|
| **Claude Sonnet 4.5** | Cursor Native | Fast | Very High | Free | Complex coding |
| **GPT-4** | Cursor Native | Medium | Very High | Free | General tasks |
| **GPT-3.5** | Cursor Native | Very Fast | High | Free | Quick tasks |
| **Gemini 1.5 Pro** | OpenRouter | Medium | Very High | $$ | Research, multimodal |
| **Gemini 1.5 Flash** | OpenRouter | Very Fast | High | $ | Fast responses |
| **Gemini 2.0 Flash** | OpenRouter | Fast | Very High | $$ | Latest features |

---

## üéØ Best Practices

### **DO:**
- ‚úÖ Use OpenRouter for accessing Gemini models
- ‚úÖ Use Cursor native models for speed
- ‚úÖ Select model based on task requirements
- ‚úÖ Use Gemini for multimodal or creative tasks
- ‚úÖ Search for models before using unknown ones

### **DON'T:**
- ‚ùå Add Gemini API key to Cursor environment variables
- ‚ùå Use expensive models for simple tasks
- ‚ùå Assume all models available without checking
- ‚ùå Ignore cost implications for high-volume usage

---

## üîß Troubleshooting

### **Issue: "Gemini model not found"**
**Solution:** Verify OpenRouter MCP is active and search available models:
```python
mcp_openrouterai_search_models(provider="google")
```

### **Issue: "API key error"**
**Solution:** OpenRouter handles authentication automatically. If errors occur, check MCP server configuration.

### **Issue: "Cursor conflicts with Gemini key"**
**Solution:** Use OpenRouter MCP instead of adding Gemini key to Cursor environment variables.

---

## üìö Integration with Other Systems

### **Pairwise Comprehensive Testing**
- Use Gemini as Reviewer model for diverse perspectives
- Gemini's strengths in analysis complement other models

### **Autonomous Development Protocol**
- Select Gemini for specific subtasks
- Use for research phase before implementation

### **Complete Everything Protocol**
- Gemini included in hybrid model selection
- Uses different provider than primary model

---

## ‚úÖ Summary

**Bottom Line:**
1. ‚úÖ **OpenRouter MCP** provides access to all Gemini models
2. ‚úÖ No need to add Gemini API key to Cursor (causes conflicts)
3. ‚úÖ Use `mcp_openrouterai_chat_completion` tool to access Gemini
4. ‚úÖ Models available: gemini-1.5-pro, gemini-1.5-flash, gemini-2.0-flash-exp
5. ‚úÖ Already configured and ready to use

**Quick Start:**
```python
# Use Gemini right now via OpenRouter
mcp_openrouterai_chat_completion(
    model="google/gemini-1.5-pro",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

**No Additional Configuration Required!** üéâ

---

## üîÑ Session Awareness

**Every session automatically knows:**
1. ‚úÖ Cursor native models are available (through Cursor settings)
2. ‚úÖ OpenRouter models including Gemini are available (via MCP)
3. ‚úÖ How to access each model
4. ‚úÖ When to use which model
5. ‚úÖ Best practices for model selection

**This information is loaded during startup and available throughout the session.**

---

**Version:** 1.0  
**Last Updated:** 2025-10-26  
**Status:** Active  
**Integration:** Complete
