# Comprehensive Testing Workflow Rules

## Overview
This rule enables Cursor AI to execute the `create_update_and_run_tests` workflow, providing comprehensive testing capabilities across unit, functional, system, and solution levels.

## Workflow Definition
When the user requests "create_update_and_run_tests" or "comprehensive testing", the AI must:

### Step 1: Research Best Practices Using Exa MCP
1. **Search Cursor Community Forum**:
   ```
   Query: "Cursor Community Forum comprehensive testing workflows unit functional system tests best practices"
   ```

2. **Search GitHub**:
   ```
   Query: "GitHub testing frameworks Jest Mocha Cypress Playwright comprehensive test automation best practices"
   ```

3. **Search for testing security**:
   ```
   Query: "testing security best practices automated testing CI/CD test isolation test data management 2024"
   ```

**Important**: Analyze research results and apply best practices to testing strategy.

### Step 2: Analyze Workspace and Detect Technologies
Scan the open workspace to identify:
- Programming languages used (JavaScript, TypeScript, Python, Java, C#, etc.)
- Existing testing frameworks (Jest, Mocha, Cypress, Playwright, pytest, etc.)
- Project structure and architecture
- Dependencies and package managers
- Build systems and CI/CD configurations

#### Technology Detection Commands
```bash
# Detect package managers
ls package.json && echo "Node.js project detected"
ls requirements.txt && echo "Python project detected"
ls pom.xml && echo "Java Maven project detected"
ls *.csproj && echo "C# project detected"

# Detect existing test frameworks
grep -r "jest\|mocha\|vitest" package.json
grep -r "pytest\|unittest" requirements.txt
grep -r "junit\|testng" pom.xml
grep -r "nunit\|xunit" *.csproj

# Analyze project structure
find . -name "*.js" -o -name "*.ts" -o -name "*.py" -o -name "*.java" -o -name "*.cs" | head -20
```

#### Framework Selection Matrix
| Language | Unit Testing | Functional Testing | E2E Testing |
|----------|-------------|-------------------|-------------|
| JavaScript/TypeScript | Jest, Vitest | Jest + Supertest | Playwright, Cypress |
| Python | pytest, unittest | pytest + requests | Selenium, Playwright |
| Java | JUnit, TestNG | JUnit + Mockito | Selenium, Playwright |
| C# | NUnit, xUnit | NUnit + Moq | Selenium, Playwright |

### Step 3: Check Existing Tests
Examine the `./unit-tests` directory (creating it if it doesn't exist) to:
- Identify existing test files
- Analyze test coverage and quality
- Determine which tests need updates
- Identify gaps in test coverage

#### Test Directory Structure
```bash
# Create test directory structure
mkdir -p ./unit-tests/{unit,functional,system,solution}
mkdir -p ./unit-tests/{fixtures,mocks,helpers}

# Check existing tests
find ./unit-tests -name "*.test.*" -o -name "*.spec.*" | sort
find ./unit-tests -name "test_*.py" | sort
find ./unit-tests -name "*Test.java" | sort
find ./unit-tests -name "*Tests.cs" | sort
```

#### Test Coverage Analysis
```bash
# Run coverage analysis (if available)
npm run test:coverage 2>/dev/null || echo "No coverage script found"
python -m pytest --cov=. 2>/dev/null || echo "No pytest coverage found"
mvn test jacoco:report 2>/dev/null || echo "No Maven coverage found"
dotnet test --collect:"XPlat Code Coverage" 2>/dev/null || echo "No .NET coverage found"
```

### Step 4: Select Appropriate Testing Framework
Based on analysis, select and configure the optimal testing framework:

#### Framework Installation Commands
```bash
# JavaScript/TypeScript
npm install --save-dev jest @testing-library/jest-dom @testing-library/react
npm install --save-dev playwright @playwright/test
npm install --save-dev supertest

# Python
pip install pytest pytest-cov pytest-mock requests
pip install playwright selenium

# Java
# Add to pom.xml: junit-jupiter, mockito-core, selenium-java

# C#
dotnet add package NUnit
dotnet add package NUnit3TestAdapter
dotnet add package Moq
dotnet add package Selenium.WebDriver
```

#### Framework Configuration
```javascript
// Jest configuration (jest.config.js)
module.exports = {
  testEnvironment: 'node',
  collectCoverage: true,
  coverageDirectory: 'coverage',
  coverageReporters: ['text', 'lcov', 'html'],
  testMatch: ['**/unit-tests/**/*.test.js'],
  setupFilesAfterEnv: ['<rootDir>/unit-tests/setup.js']
};
```

### Step 5: Create Unit Tests
Create comprehensive unit tests for each method/function:
- Test individual functions and methods
- Cover edge cases and error conditions
- Include positive and negative test cases
- Ensure proper test isolation and mocking
- Follow AAA pattern (Arrange, Act, Assert)

#### Unit Test Template (JavaScript/Jest)
```javascript
// ./unit-tests/unit/example.test.js
describe('Example Function', () => {
  beforeEach(() => {
    // Setup before each test
  });

  afterEach(() => {
    // Cleanup after each test
  });

  describe('positive cases', () => {
    it('should return expected result for valid input', () => {
      // Arrange
      const input = 'valid input';
      const expected = 'expected output';

      // Act
      const result = exampleFunction(input);

      // Assert
      expect(result).toBe(expected);
    });
  });

  describe('negative cases', () => {
    it('should throw error for invalid input', () => {
      // Arrange
      const invalidInput = null;

      // Act & Assert
      expect(() => exampleFunction(invalidInput)).toThrow('Invalid input');
    });
  });

  describe('edge cases', () => {
    it('should handle empty string', () => {
      // Arrange
      const emptyInput = '';

      // Act
      const result = exampleFunction(emptyInput);

      // Assert
      expect(result).toBeDefined();
    });
  });
});
```

### Step 6: Create Functional Tests
Create functional tests that:
- Test interactions between multiple methods
- Verify component integration
- Test API endpoints and data flow
- Validate business logic workflows
- Ensure proper error handling

#### Functional Test Template (JavaScript/Supertest)
```javascript
// ./unit-tests/functional/api.test.js
const request = require('supertest');
const app = require('../../src/app');

describe('API Functional Tests', () => {
  describe('POST /api/users', () => {
    it('should create user and return user data', async () => {
      // Arrange
      const userData = {
        name: 'John Doe',
        email: 'john@example.com'
      };

      // Act
      const response = await request(app)
        .post('/api/users')
        .send(userData)
        .expect(201);

      // Assert
      expect(response.body).toHaveProperty('id');
      expect(response.body.name).toBe(userData.name);
      expect(response.body.email).toBe(userData.email);
    });

    it('should validate required fields', async () => {
      // Arrange
      const invalidData = { name: 'John' }; // Missing email

      // Act & Assert
      await request(app)
        .post('/api/users')
        .send(invalidData)
        .expect(400);
    });
  });
});
```

### Step 7: Create System Tests
Create system tests that:
- Test entire modules or subsystems
- Verify end-to-end functionality
- Test database interactions
- Validate external service integrations
- Ensure system performance and reliability

#### System Test Template (Playwright)
```javascript
// ./unit-tests/system/e2e.test.js
const { test, expect } = require('@playwright/test');

test.describe('System E2E Tests', () => {
  test.beforeEach(async ({ page }) => {
    await page.goto('http://localhost:3000');
  });

  test('complete user registration flow', async ({ page }) => {
    // Arrange
    const userData = {
      name: 'John Doe',
      email: 'john@example.com',
      password: 'securePassword123'
    };

    // Act
    await page.fill('[data-testid="name-input"]', userData.name);
    await page.fill('[data-testid="email-input"]', userData.email);
    await page.fill('[data-testid="password-input"]', userData.password);
    await page.click('[data-testid="register-button"]');

    // Assert
    await expect(page.locator('[data-testid="success-message"]')).toBeVisible();
    await expect(page.locator('[data-testid="user-dashboard"]')).toBeVisible();
  });
});
```

### Step 8: Create Solution Tests
Create solution tests that:
- Test the complete application
- Verify user workflows and scenarios
- Test cross-platform compatibility
- Validate security and authentication
- Ensure scalability and performance

#### Solution Test Template (Comprehensive E2E)
```javascript
// ./unit-tests/solution/complete-app.test.js
const { test, expect } = require('@playwright/test');

test.describe('Complete Application Solution Tests', () => {
  test('full user journey from registration to logout', async ({ page }) => {
    // Complete user journey test
    await page.goto('http://localhost:3000');
    
    // Registration
    await page.click('[data-testid="register-link"]');
    await page.fill('[data-testid="name-input"]', 'John Doe');
    await page.fill('[data-testid="email-input"]', 'john@example.com');
    await page.fill('[data-testid="password-input"]', 'securePassword123');
    await page.click('[data-testid="register-button"]');
    
    // Verify registration success
    await expect(page.locator('[data-testid="success-message"]')).toBeVisible();
    
    // Login
    await page.fill('[data-testid="email-input"]', 'john@example.com');
    await page.fill('[data-testid="password-input"]', 'securePassword123');
    await page.click('[data-testid="login-button"]');
    
    // Verify dashboard access
    await expect(page.locator('[data-testid="user-dashboard"]')).toBeVisible();
    
    // Perform user actions
    await page.click('[data-testid="create-task"]');
    await page.fill('[data-testid="task-title"]', 'Test Task');
    await page.fill('[data-testid="task-description"]', 'Test Description');
    await page.click('[data-testid="save-task"]');
    
    // Verify task creation
    await expect(page.locator('[data-testid="task-list"]')).toContainText('Test Task');
    
    // Logout
    await page.click('[data-testid="logout-button"]');
    await expect(page.locator('[data-testid="login-form"]')).toBeVisible();
  });
});
```

### Step 9: Run Tests and Handle Failures
Execute all tests with intelligent failure handling:
- Run tests in logical order (unit → functional → system → solution)
- Stop on first failure and analyze the issue
- Attempt to fix failing tests automatically
- Log all test results and failures
- Continue until all tests pass or AI cannot resolve issues

#### Test Execution Script
```bash
#!/bin/bash
# ./unit-tests/run-tests.sh

echo "Starting comprehensive test suite..."
echo "Timestamp: $(date)" > ./unit-tests/test-results.log

# Create test results directory
mkdir -p ./unit-tests/results

# Run tests in order
echo "Running Unit Tests..."
npm test -- --testPathPattern=unit-tests/unit --verbose >> ./unit-tests/test-results.log 2>&1
if [ $? -ne 0 ]; then
    echo "Unit tests failed. Analyzing and attempting fixes..."
    # AI analyzes failure and attempts fixes
    exit 1
fi

echo "Running Functional Tests..."
npm test -- --testPathPattern=unit-tests/functional --verbose >> ./unit-tests/test-results.log 2>&1
if [ $? -ne 0 ]; then
    echo "Functional tests failed. Analyzing and attempting fixes..."
    # AI analyzes failure and attempts fixes
    exit 1
fi

echo "Running System Tests..."
npx playwright test ./unit-tests/system --reporter=html >> ./unit-tests/test-results.log 2>&1
if [ $? -ne 0 ]; then
    echo "System tests failed. Analyzing and attempting fixes..."
    # AI analyzes failure and attempts fixes
    exit 1
fi

echo "Running Solution Tests..."
npx playwright test ./unit-tests/solution --reporter=html >> ./unit-tests/test-results.log 2>&1
if [ $? -ne 0 ]; then
    echo "Solution tests failed. Analyzing and attempting fixes..."
    # AI analyzes failure and attempts fixes
    exit 1
fi

echo "All tests passed successfully!"
```

### Step 10: Optimize and Document
- Optimize test performance and execution time
- Ensure minimal code changes while maintaining functionality
- Prioritize user experience in all fixes
- Document test coverage and results
- Create comprehensive test reports

## Key Principles
1. **Minimize Code Changes**: Only make necessary changes to fix issues
2. **Prioritize User Experience**: Ensure fixes improve user experience
3. **Optimize Code**: Always optimize for performance and maintainability
4. **Comprehensive Coverage**: Test all levels from unit to solution
5. **Security First**: Include security testing in all test levels
6. **Documentation**: Maintain comprehensive test documentation

## Success Criteria
The workflow is complete when:
- ✅ All unit tests pass with >90% coverage
- ✅ All functional tests pass with proper integration
- ✅ All system tests pass with E2E validation
- ✅ All solution tests pass with complete user journeys
- ✅ Performance benchmarks are met
- ✅ Security tests validate protection measures
- ✅ Test documentation is comprehensive
- ✅ Test execution is optimized for speed

## Error Handling
When tests fail:
1. Analyze the failure output
2. Identify the root cause
3. Attempt automatic fixes for common issues
4. Log detailed failure information
5. Continue with remaining tests
6. Provide comprehensive failure report

## Integration with Existing Workflows
This testing workflow integrates with:
- AWS deployment workflows
- Mobile app deployment workflows
- Web application deployment workflows
- Database migration workflows

The AI must ensure all deployed applications have comprehensive test coverage before deployment completion.